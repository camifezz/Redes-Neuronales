# Redes-Neuronales - TP1

## Tabla de Contenidos
- [Introducci√≥n](#Introducci√≥n)
- [Red recurrente](#Red-recurrente)
- [Modelo de Hopfield](#Modelo-de-Holpfield)
- [Trabajo Pr√°ctico - Parte 1](#Trabajo-Pr√°ctico-Parte-1)
    - [Punto a](#a--verifique-si-la-red-aprendi√≥-las-im√°genes-ense√±adas)
    - [Punto b](#b--eval√∫e-la-evoluci√≥n-de-la-red-al-presentarle-versiones-alteradas-de-las-im√°genesa-prendidas-agregado-de-ruido-elementos-borrados-o-agregados)
    - [Punto c](#c--eval√∫e-la-existencia-de-estados-espurios-en-la-red-patrones-inversos-y-combinaciones-de-un-n√∫mero-impar-de-patrones)
    - [Punto d](#d--realice-un-entrenamiento-con-las-6-im√°genes-disponibles-es-capaz-la-red-de-aprender-todas-las-im√°genes-explique)

- [Trabajo Pr√°ctico - Parte 2](#Trabajo-Pr√°ctico---Parte-2)
    - [Punto a](#a--comprobar-estad√≠sticamente-la-capacidad-de-la-red-de-hopfield-82-calculando-la-cantidad-m√°xima-de-patrones-pseudo-aleatorios-aprendidos-en-funci√≥n-del-tama√±o-de-la-red)
    - [Punto b](#b--proponga-una-manera-de-generar-patrones-con-distintos-grados-de-correlaci√≥n)
- [Trabajo Pr√°ctico - extra](#Trabajo-Pr√°ctico--extra)


## Introducci√≥n
### ¬øQu√© es una red neuronal biol√≥gica y como se imitan mediante algoritmos?
üß† `Red neuronal biol√≥gica`: funciona con neuronas, impulsos el√©ctricos y sinapsis. Cada neurona decide si transmite una se√±al en funci√≥n de la suma de sus entradas y la fuerza de las conexiones. El aprendizaje ocurre reforzando o debilitando esas conexiones.

üíª `Red neuronal artificial`: est√° compuesta por "nodos" o "neuronas artificiales" que realizan operaciones matem√°ticas simples.

Las redes neuronales artificiales imitan de manera muy simplificada la forma en que las redes biol√≥gicas procesan y aprenden informaci√≥n. No replican la complejidad del cerebro, pero toman la idea de "muchas unidades simples conectadas en red que, al trabajar juntas, generan comportamientos complejos".

Para que una red neuronal pueda funcionar correctamente necesita pasar por un proceso de entrenamiento, en el cual se ajustan los pesos de sus conexiones entre capas. Mediante la exposici√≥n a distintos conjuntos de datos, el sistema va perfeccionando su capacidad de aprendizaje, logrando que las neuronas artificiales reconozcan patrones y resuelvan problemas, ofreciendo respuestas que se asemejan a las de un ser humano.

## Red recurrente
Una red recurrente es aquella en donde las neuronas tienen conexiones que vuelven sobre s√≠ mismas o hacia otras neuronas en la misma capa, lo que permite que el sistema ‚Äúrecuerde‚Äù estados anteriores.

La red de Hopfield es una de ellas. En la pr√≥xima secci√≥n se hace una introducci√≥n al modelo

## Modelo de Hopfield
El modelo de Hopfield, propuesto por John Hopfield en 1982, es una de las primeras arquitecturas de redes neuronales artificiales que busc√≥ imitar el funcionamiento de la memoria en el cerebro

La idea principal es que la red pueda funcionar como una `memoria asociativa`:
- Durante el entrenamiento, se almacenan patrones en los pesos de la red.

- Cuando se le presenta un patr√≥n incompleto o ruidoso, la red evoluciona din√°micamente hasta estabilizarse en el patr√≥n correcto o en uno muy cercano.

#### Memoria asociativa
La memoria asociativa es la capacidad de un sistema de recordar informaci√≥n completa a partir de fragmentos o datos incompletos.
En el cerebro biol√≥gico ser√≠a, por ejemplo: ver solo una parte de una foto y aun as√≠ reconocer a la persona que aparece.

En una red de Hopfield, esta idea se implementa almacenando patrones en los pesos de las conexiones.

La memoria asociativa en Hopfield permite que la red act√∫e como un recordador de patrones, reconociendo informaci√≥n parcial y llev√°ndola al patr√≥n m√°s parecido que ya ten√≠a guardado.

## Trabajo Pr√°ctico - Parte 1
## Entrene una red de Hopfield ‚Äò82 con las im√°genes binarias disponibles en el campus.
### a- Verifique si la red aprendi√≥ las im√°genes ense√±adas.
Para probar esto, le presento a la red 3 im√°genes del mismo tama√±o y eval√∫o la convergencia de cada imagen.

![paloma](paloma-entrenamiento.png)
![quijote](quijote-entrenamiento.png)
![torero](torero-entrenamiento.png)

La imagen recuperada es la misma que el patr√≥n inicial. Por lo que podemos concluir que la red aprendi√≥ de los patrones ense√±ados.

### b- Eval√∫e la evoluci√≥n de la red al presentarle versiones alteradas de las im√°genesa prendidas: agregado de ruido, elementos borrados o agregados.

### Agregandole ruido a los patrones invirtiendo los bits
Para este punto eleg√≠ las im√°genes de tama√±o (50,50), les agregu√© diferentes grados de ruido y prob√© la red.
Eleg√≠ agregarle ruido invirtiendo bits de forma aleatoria.

Se puede ver que la red es bastante tolerante al ruido elegido, ya que logr√≥ converger a la imagen inicial en los 3 casos.

![panda](panda-con-ruido.png)
![v](v-con-ruido.png)
![perro](perro-con-ruido.png)



### Agregandole ruido a los patrones borrando diferentes porcentajes del patr√≥n

Se puede observar que borrando hasta un 70% del patr√≥n original la red converge de forma correcta.


#### 1- Borrando el 50% del patr√≥n original
![panda-borrado](panda-borrado-50.png)

#### 2- Borrando el 70% del patr√≥n original
![panda-borrado](panda-borrado-70.png)

#### 3- Borrando el 90% del patr√≥n original
![panda-borrado](panda-borrado-90.png)

### c- Eval√∫e la existencia de estados espurios en la red: patrones inversos y combinaciones de un n√∫mero impar de patrones.
#### Estados espurios

Una red de Hopfield es una red de memoria asociativa: entrena ciertos patrones ŒæŒº como m√≠nimos de energ√≠a.
Cuando a la red se le da una entrada ruidosa o incompleta, la din√°mica baja la ‚Äúenerg√≠a‚Äù y cae en el m√≠nimo m√°s cercano es decir, recupera el patr√≥n original.
Adem√°s de estos patrones entrenados, aparecen otros m√≠nimos estables que no son patrones originales, a esos m√≠nimos se los llama estados espurios.

#### Tipos de estados espurios
##### 1- Patrones inversos
Si ŒæŒº es un patr√≥n entrenado, su inverso -ŒæŒº tambi√©n puede ser un m√≠nimo estable.
Por ejemplo: Si la red memoriz√≥ un panda, la red puede converger tambi√©n en un "anti panda" todo invertido lo blaco/negro.
##### 2- Combinaciones de un n√∫mero impar de patrones
Un estado espurio tambi√©n puede ser una suma de k (n√∫mero impar, observar en la formula 2k+1) estados ense√±ados:

$$
s = \operatorname{sign}\!\left( \xi^{\mu_1} + \xi^{\mu_2} + \cdots + \xi^{\mu_{2k+1}} \right)
$$


Es decir, si se entren√≥ la red con tres im√°genes distintas, la red puede generar una especie de mezcla h√≠brida de esas tres y tratar de recordarla como si fuera un patr√≥n real.

Visualmente, esas combinaciones suelen parecer ‚Äúfantasmas‚Äù de varios patrones mezclados.

#### Como podemos ver esto en la practica
- Si la red se inicializa con un patr√≥n muy ruidoso, en vez de converger al patr√≥n correcto, puede caer en uno de estos estados espurios.
- A medida que se sobrecarga la red, los espurios se vuelven m√°s frecuentes y limitan la utilidad de Hopfield.

Teniendo en cuenta lo anteriomente explicado, voy a evaluar la existencia de estados espurios en la red ya entrenada.

Elijo los patrones de tama√±o (50x50) ya que tienen mas detalle de color.

Lo que se hace es:
1. Entreno a la red con los patrones reales.

2. Le doy como entrada un patr√≥n inverso (que nunca fue parte del entrenamiento).

Si la red:

1. Devuelve el patr√≥n real entonces el inverso no es estable.

2. Si converge al patr√≥n inverso entonces ese inverso se convirti√≥ en un estado espurio.

### Patrones inversos
![panda-inverso](panda-inverso.png)
![perro-inverso](perro-inverso.png)
![v-inverso](v-inverso.png)

Como pudimos observar estos patrones inversos son estados espurios de la red. La red recuerda tambi√©n la versi√≥n invertida aunque no le haya ense√±ado antes porque matem√°ticamente tiene la misma coherencia interna que el patr√≥n real.

### Combinaciones de un n√∫mero impar de patrones
![combinacion-3-patrones](combinacion-3-patrones.png)
Decid√≠ tomar una combinaci√≥n impar de patrones que no estaba en el set de entrenamiento y podemos observar que la red lo devolvi√≥ tal cual y no coincide con ninguno de los patrones ense√±ados. Por lo que se puede concluir que es un estado espurio del tipo "combinaci√≥n impar de patrones"

### d- Realice un entrenamiento con las 6 im√°genes disponibles. ¬øEs capaz la red de aprender todas las im√°genes? Explique.
Para realizar el entrenamiento con todas las imagenes, lo primero que decido hacer en normalizarlas a todas en un mismo tama√±o de 50x50 completando con pixeles blancos.

![todo_1](todo_1.png)
![todo-2](todo-2.png)
![todo-3](todo-3.png)
![todo-4](todo-4.png)
![todo-5](todo-5.png)
![todo-6](todo-6.png)

Se puede observar que luego de presentarse los 6 patrones con los que la red aprendi√≥, no pudo aprender bien todas las im√°genes, ya que en 4 casos no logr√≥ converger al patr√≥n real.

Este comportamiento refleja las restricciones de las redes de Hopfield: su memoria es aproximada y no garantiza la recuperaci√≥n perfecta de todos los patrones entrenados, especialmente si son visualmente parecidos o si se introducen en gran cantidad, ya que posiblemente se haya alcanzado su capacidad m√°xima de almacenamiento.

## Trabajo Pr√°ctico - Parte 2

### a- Comprobar estad√≠sticamente la capacidad de la red de Hopfield ‚Äò82 calculando la cantidad m√°xima de patrones pseudo-aleatorios aprendidos en funci√≥n del tama√±o de la red.
### Obtener experimentalmente los resultados de la siguiente tabla (los valores de la tabla corresponden a una iteraci√≥n con actualizaci√≥n sincr√≥nica).

| P<sub>error</sub> | P<sub>max</sub> / N
|:------------------|:--------------:
| 0,001             | 0,105       
| 0,0036       | 0,138    
|0,01           | 0,185
|0,05           | 0,37
|0,1            | 0,61

En este experimento usamos una red de Hopfield ‚Äô82 para estimar su capacidad de almacenamiento. Para eso generamos patrones pseudo-aleatorios y medimos hasta cu√°ntos se pueden guardar antes de superar un cierto nivel de error P<sub>error</sub>.

La elecci√≥n de los par√°metros es la siguiente:
- El par√°metro `n` fija el tama√±o de la red. Tom√© como valor `n=10`, se obtuvieron `N=100 neuronas`. Esto significa que cada patr√≥n es un vector de 100 bits en (-1;1). Prob√© con un valor de `n` mayor y el programa se quedaba colgado.
- El par√°metro `trials` indica cu√°ntas veces repetimos el experimento para el mismo umbral de error. Cada repetici√≥n genera patrones aleatorios distintos, por lo que los resultados pueden variar. Se repite muchas veces y se promedian los resultados para llegar a estimaciones mas estables de la capacidad de la red.
- El par√°metro `seed` controla la aleatoriedad. Si no lo fijamos, cada corrida puede dar resultados diferentes porque los patrones iniciales cambian.

Los cambios implementados en las funciones usadas en la parte 1 son las siguientes:
- Patrones (pseudo-aleatorios en vez de im√°genes).

- Regla de pesos (dividir por ùëÅ en vez de P).

- Evaluaci√≥n (una pasada s√≠ncrona + tasa de error en bits, en vez de convergencia).



Los resultados obtenidos con los par√°metros seteados como se muestran a continuaci√≥n son los siguientes:
- `n=10`
- `trials=100`
- `seed=42`


| P<sub>error</sub> | P<sub>max</sub> / N
|:------------------|:--------------:
| 0,001             | 0.1155       
| 0,0036       | 0.1410    
|0,01           | 0.1840
|0,05           | 0.3770
|0,1            | 0.6140

Estos resultados muestran valores bastantes cercanos a los que se muestran en la tabla inicial, por lo que se puede concluir que aunque se tenga una red con pocas neuronas los resultados siguen a la teor√≠a.

### b- Proponga una manera de generar patrones con distintos grados de correlaci√≥n.
### Utilice el m√©todo propuesto para analizar c√≥mo var√≠a la capacidad de la red de Hopfield en funci√≥n de la correlaci√≥n entre patrones.

#### M√©todo propuesto
El m√©todo propuesto tiene como inicio un patr√≥n base aleatorio de longitud N con valores (-1,1).

Para cada nuevo patr√≥n, se recorre cada posici√≥n del vector y se decide, con una probabilidad `proba`, si ese bit se invierte de signo o se deja igual.

As√≠ obtenemos un conjunto de patrones donde todos est√°n correlacionados con el patr√≥n base, y el grado de correlaci√≥n est√° dado por `proba`

- Cuanto menor sea el valor del par√°metro `proba` m√°s se van a parecer los patrones entre si. (alta correlaci√≥n)
- Cuanto mayor sea el valor del par√°metro `proba` mas diferentes van a ser los patrones.
```
def generar_patrones_correlados(P, N, proba=0.1, rng=np.random.default_rn()):
    """
    Genera P patrones de longitud N con correlaci√≥n controlada.
    proba: probabilidad de cambiar cada bit respecto al patr√≥n base.
    """
    base = rng.choice([-1, 1], size=N)   # patr√≥n base aleatorio
    patterns = [base]
    for _ in range(P-1):
        mask = rng.random(N) < proba
        nuevo = base.copy()
        nuevo[mask] *= -1
        patterns.append(nuevo)
    return np.array(patterns)
```

Usando las mismas funciones que se usaron en el punto 2a para calcular la capacidad de la red, se arma una tabla para analizar c√≥mo var√≠a la capacidad de la red de Hopfield en funci√≥n de la correlaci√≥n entre patrones. Los valores se obtienen con una red de N=1000 (mil neuronas)

| proba | P<sub>max</sub> / N
|:------------------|:--------------:
|0.00 | 0.099
| 0.05     | 0.002    
|0.10           | 0.002
|0.20	           | 0.002
|0.50           | 0.099

Se puede observar que la capacidad de la red disminuye cuando los patrones presentan alta correlaci√≥n entre s√≠, es decir, cuando se aumenta el valor de `proba`. 

Lo √∫nico que no termino de interpretar es porque con `proba=0.50` pega un salto al valor `0.099`.


## Trabajo Pr√°ctico - extra
### Implemente una red de Hopfield ‚Äò82 que aprenda patrones pseudo-aleatorios y estudie qu√© sucede con los patrones aprendidos cuando algunas interconexiones son eliminadas al azar.

Usando las funciones ya implementadas para toda la parte 2 anterior, laa idea practica consta de los siguientes pasos:
1. Generar un conjunto de patrones pseudoaleatorios.
2. Calcular la matriz de pesos W con la funci√≥n de Hopfield.
3. Eliminar al azar un porcentaje de interconexiones (poner en cero algunos pesos de W).
4. Ver si los patrones siguen siendo estables (se recuperan despu√©s de una pasada sincr√≥nica).

#### 1. ¬øC√≥mo cambia el error en funci√≥n del porcentaje de sinapsis eliminadas?
Con 100 neuronas y 12 patrones diferentes que se est√°n almacenando en la red, se puede ver en el gr√°fico que a mayor porcentaje de sinapsis borrada mayor es el error.

Entre un 60% - 70% de sinapsis eliminada el error est√° muy cerca de cero, pero cuando se borra un porcentaje mayor al 70%, el error crece casi de forma exponencial.
![error-vs-borrado](error-vs-borrado.png)

#### 2. ¬øC√≥mo cambia la capacidad en funci√≥n del porcentaje de sinapsis eliminadas?
Con 100 neuronas y 12 patrones diferentes que se est√°n almacenando en la red, se puede ver en el gr√°fico que, en lineas generales, a mayor porcentaje de sinapsis borrada la capacidad de la red disminuye.

![capacidad-vs-borrado](capacidad-vs-borrado.png)








