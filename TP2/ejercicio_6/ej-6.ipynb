{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df2e2be",
   "metadata": {},
   "source": [
    "## 6. Entrene una red convolucional para clasificar las imágenes de la base de datos MNIST. ¿Cuál es la red convolucional más pequeña que puede conseguir con una exactitud de al menos 90% en el conjunto de evaluación? ¿Cuál es el perceptrón multicapa más pequeño que puede conseguir con la misma exactitud?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde9c71",
   "metadata": {},
   "source": [
    "Podemos empezar con una red como la del ejemplo hasta encontrar la mas chica (a nivel parámetros). Lograr el desempeño y \n",
    "ver que puedo sacarle sin que el desempeño caiga (a nivel parámetros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d662798",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Definicion de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f13baeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c293274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de entrada brindada por la cátedra.\n",
    "# La clase hereda de Module\n",
    "class conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0) # capa convolucional (usa filtros no neuronas)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 2da capa convolucional (usa filtros no neuronas)\n",
    "        self.fc1 = nn.Linear(400, 120) # primera capa. 120 neuronas\n",
    "        self.fc2 = nn.Linear(120, 84)  # capa intermedia . 84 neuronas\n",
    "        self.fc3 = nn.Linear(84, 10)   # capa de salida\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e81898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan a memoria los tensores de train/test (imágenes y etiquetas) desde archivos .pt\n",
    "# Me devuelve en cada caso un tensor -> al tensor lo puedo pensar arreglo (array) n-dimensional:\n",
    "\n",
    "# 0-D: escalar (un número)\n",
    "# 1-D: vector (ej. shape [10])\n",
    "# 2-D: matriz (ej. [32, 10])\n",
    "# 3-D+: tensores con más ejes (imágenes, videos, batches, etc.)\n",
    "\n",
    "#Imágenes → features que entran a la red.\n",
    "#Labels → objetivo/“respuesta correcta” para calcular la pérdida y guiar el aprendizaje.\n",
    "\n",
    "device = 'cpu'\n",
    "training_data = torch.load('MNIST_training_data.pt').to(torch.float32).to(device)\n",
    "training_labels = torch.load('MNIST_training_labels.pt').to(device)\n",
    "test_data = torch.load('MNIST_test_data.pt').to(torch.float32).to(device)\n",
    "test_labels = torch.load('MNIST_test_labels.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "047b13c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esto me devuelve: torch.Size([60000, 28, 28])\n",
    "# 60000 → cantidad de imágenes (todo el set de entrenamiento de MNIST).\n",
    "#28, 28 → alto y ancho de cada imagen (en grises).\n",
    "\n",
    "training_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96458ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar tipos/shape antes (una sola vez):\n",
    "training_labels = training_labels.long()\n",
    "test_labels     = test_labels.long()\n",
    "# Si tus imágenes están [N,28,28], agregá canal:\n",
    "if training_data.ndim == 3:\n",
    "    training_data = training_data.unsqueeze(1)\n",
    "    test_data     = test_data.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17c116ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_loss = nn.CrossEntropyLoss() # función de perdida\n",
    "modelo = conv().to(device)\n",
    "optimizer = torch.optim.SGD(modelo.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a98cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minibatches(x, y, batch_size: int, shuffle: bool = True):\n",
    "    \"\"\"\n",
    "    Devuelve un DataLoader que entrega (xb, yb) ya en mini-batches.\n",
    "    x: Tensor de imágenes con shape [N, C, H, W]\n",
    "    y: Tensor de labels con shape [N]\n",
    "    \"\"\"\n",
    "    ds = TensorDataset(x, y)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56b013aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [6, 3, 5, 5], expected input[128, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m xb = xb.to(device)\n\u001b[32m     13\u001b[39m yb = yb.long().to(device)            \u001b[38;5;66;03m# CrossEntropy: labels en long\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodelo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss = f_loss(outputs, yb)\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Redes Neuronales/Redes-Neuronales/TP2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Redes Neuronales/Redes-Neuronales/TP2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mconv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     15\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     16\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Redes Neuronales/Redes-Neuronales/TP2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Redes Neuronales/Redes-Neuronales/TP2/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Redes Neuronales/Redes-Neuronales/TP2/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Redes Neuronales/Redes-Neuronales/TP2/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [6, 3, 5, 5], expected input[128, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# La idea es entrenar en mini-batches a lo largo de varias \"epocas\"\n",
    "# La epoca (epoch) es una pasada completa por todo el dataset de entrenamiento.\n",
    "N_epocas = 10\n",
    "minibatch_size = 128\n",
    "\n",
    "for epoca in range(N_epocas):\n",
    "    modelo.train()\n",
    "    train_loader = make_minibatches(training_data, training_labels, minibatch_size, shuffle=True)\n",
    "\n",
    "    total_loss, total_ej = 0.0, 0\n",
    "    for xb, yb in train_loader:              # <-- acá viene cada mini-batch\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.long().to(device)            # CrossEntropy: labels en long\n",
    "\n",
    "        outputs = modelo(xb)\n",
    "        loss = f_loss(outputs, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_ej   += xb.size(0)\n",
    "\n",
    "    print(f\"Época {epoca+1}: loss_prom={total_loss/total_ej:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1c1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
